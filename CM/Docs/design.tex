\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{alltt,amsmath,hyperref,graphicx}
\usepackage[usenames,dvipsnames]{xcolor}

% Link colors
\hypersetup{colorlinks=true,linkcolor=black,urlcolor=blue,citecolor=OliveGreen}

% Set paragraph indentation
\parindent 0pt
\parskip 1.0ex plus 0.5ex minus 0.2ex

\title{Concurrency and Multithreading - Design}
\author{Jayke Meijer, 2526284, jmr251, \url{jayke.meijer@gmail.com} \\
Tadde√ºs Kroes, 2526902, tks590, \url{taddeuskroes@gmail.com}}

\def\bigoh{\mathcal{O}}

\begin{document}

\maketitle

\tableofcontents
\pagebreak

\section{Introduction}

This document discusses the design of several concurrent data
structures. Both an advanced design and a discussion about the
expected performance will be given for each data structure.

The data structures are the following: CoarseGrainedList,
CoarseGrainedTree, FineGrainedList, FineGrainedTree,
LockFreeList and LockFreeTree.

The difficulty with these data structures is that they are
accessed by several threads concurrently, so mutual exclusion has
to be provided for the parts where the updates take place.

\section{CoarseGrainedList}

The first data structure discussed is the CoarseGrainedList.
This is a singly-linked list. When an element is added or removed,
the entire data structure is locked.

\subsection{Design}

For the CoarseGrainedList a singly-linked list is required. To create this
list a sequence of nodes is used. These nodes contain both the stored items
and a link to the next node in the list.

To lock the list when an \emph{add} or \emph{remove} operation is performed,
a single lock is required. The lock is acquired in the beginning of the
operation. Once the lock is obtained, either the position to insert the new
node or the node to remove is located, and the remove or insert operation is
performed. Once this is done, the lock is released, allowing
other threads to do their operations.

\subsection{Performance}

The performance of the CoarseGrainedList is expected to be very poor. Almost
no parallel execution is possible, due to the fact that the entire
data structure is locked during the entire add or remove operation. This means
that whenever one thread is performing such an operation, no other thread can
do such an operation, but also not even prepare this operation, by for example
locating the nodes affected.

This means that regardless of the number of elements in the data structure,
only one thread at a time can access the list. The performance is therefore
expected to be very poor when a lot of threads are trying to execute a lot
of operations on the list.

The complexity for a single thread will be $\bigoh(n)$ with $n$ being
the number of elements in the list, since
locating the affected nodes means in worst case looping through the entire
list.

Since no parallel execution of the operations is possible, the worst-case
complexity of the entire algorithm is expected to be $\bigoh(n * m)$,
with $n$ the number of elements and $m$ the number of threads.

\section{CoarseGrainedTree}

The CoarseGrainedTree is a binary search tree. As with the
CoarseGrainedList, the complete data structure is locked when
an add or remove is performed.

\subsection{Design}

The design of the CoarseGrainedTree is similar to the design of the
CoarseGrainedList. However, the nodes used now have up to two child nodes
instead of a single following node.

The locking mechanism remains the same: the entire tree is locked before
an addition or removal operation is performed. Again, the operation performed
during the lock includes the act of locating the affected nodes.

For the addition and removal of nodes we use standard BST algorithms
\cite{insert} and \cite{delete}.

\subsection{Performance}

As with the CoarseGrainedList, the performance will be poor when a lot of
threads try to perform operations simultaneously.

The act of locating affected nodes will be faster, since we are dealing
with a binary tree instead of a list. Therefore the overall performance
will be better, due to the fact that the basic algorithm is more effective.

The complexity of a single thread is in the worst-case the same as with the
linked list, when the tree is completely unbalanced, so $\bigoh(n)$.
However, the average complexity will be better than that, and better than the
average complexity of the linked list.

If the tree is completely balanced, the complexity will be
$\bigoh(\log n)$.

Once again, these complexities are to be multiplied with the number of
threads, giving $\bigoh(n * m)$ and $\bigoh(\log(n) * m)$
respectively.

\section{FineGrainedList}

The FineGrainedList is a linked list that only locks part of the data
structure when performing an addition or removal operation. This allows
multiple threads to update the list at the same time.

\subsection{Design}

Locking is done as follows. A node is locked before its value is checked.
If this node is not the required node, the next node is locked after which the
lock on the current node is released. This way it is guaranteed that a node
can not be changed between checking its value and the actual operation on the
node. This is called \emph{Hand-over-hand} locking.

\subsection{Performance}

The advantage of this data structure is expected to be better than that of the
CoarseGrainedList, since multiple threads can operate on the list at the same
time.

However, there are some remarks to be made. The result of the 
\emph{hand-over-hand} locking is that a thread can not pass another thread
while traversing the list. This causes a delay when elements are added/removed
in ascending order. Each thread will then still have to wait on the thread in
front of him. The only advantage over the CoarseGrainedList is that while
it is waiting for the other list, it is already on its way to the proper node.

The advantage of the FineGrainedList as opposed to the CoarseGrainedList will
probably grow with the size of the list. In the FineGrainedList, part of an
operation can be performed before a locked element is encountered, whereas the
CoarseGrainedList blocks the list altogether.

However, when no two threads are accessing the list at the same time, the
FineGrainedList will have some lock overhead because elements are locked
individually. Since the test application does not add pauses between
adding/removing different elements, this case will probably not occur in the
benchmark results.

\section{FineGrainedTree}

The FineGrainedTree is a tree in which only a part of the data structure
will be locked for add or remove operations, as with the FineGrainedList.

\subsection{Design}

The design of the lock mechanism in the FineGrainTree is similar to that of the
FineGrainList. The next node visited is locked before the lock on the current
node can be released. The difference is that the current node locks one of its
child nodes next instead of the singly-linked next node.

\subsection{Performance}

With the FineGrainedList the problem is that while traversing the list, one thread has to
wait for other threads that perform their tasks at some node with a lower value. In a tree
structure, this happens less often. When the thread A operates in another subtree than
thread B, A and B not interfere with each other. Thus, the tree structure lowers the
possibility of a ``congestion'' during data structure traversal of simultaneous threads.

This effect is strongest when the tree is well balanced and contains a significant number
of elements. Like with the CoarseGrainedTree, if the tree is completely unbalanced it
basically is a linked list. The advantage of the tree structure is then lost. When the
tree contains a large number of elements, the change of a congestion occurring at a locked
element is smaller than in a tree containing fewer elements.

To maximize the advantage of a tree structure as opposed to a linked list, the
number of threads should be small in comparison to the number of elements in the
data structure. If the tree is balanced to some degree due to the randomness of the
elements added to the tree, there should be minimum interference between threads.

The root of the tree and first few levels of nodes can be expected to be a bottleneck,
since these will be visited (and thus locked) by each thread for every operation.

\section{LockFreeList}

The LockFreeList is a linked list that does not use a lock when updating
the datastructure. Instead it uses a single atomic operation for both
comparing and updating, compare-and-set instructions.

\subsection{Design}

The LockFreeList operations traverse through the linked list in pairs of nodes, like
the FineGrainedList does. However, whereas the FineGrainedList locks a node before
reading its value, the LockFreeList does not. To still be able to guarantee that a
node of which the value has been read was not removed by another thread, all nodes
have a mark. The mark indicates whether a node has been logically removed from the
list (but not physically). The removal operation will also attempt to remove the node
physically. When this fails, the physical removal is done by the next thread that iterates over the node,
using an abstraction of the pair-wise iteration called a \emph{Window}. A Window iterates
over the list starting from its head, and returns two nodes that are logically in the
list. Any nodes that are marked for removal which are encountered during this iteration,
are physically removed from the list. This removal is achieved using an atomic
\texttt{compareAndSet} instruction, so that no two threads can be removing the same node 
at the same time. When a thread attempts to
remove a node that is already being removed by another thread, it starts iterating back
at the head of the list (because its reference to the next node has become invalid).

Apart from usage for traversal, the \texttt{compareAndSet} instruction is also used
to update references from nodes to their successors. The instruction insures that the
reference is not changed by another thread in between reading its value and updating
it.

\subsection{Performance}

In relation to the FineGrainedList, the LockFreeList should perform much better.
While the FineGrainedList can cause a congestion at a locked element in the list,
the LockFreeList allows threads to keep traversing through the list past nodes on
which operations are being performed. This advantage is especially useful for lists
with a large number of elements, in which case traversal covers a large part of the
time needed for an operation.

The LockFreeList should also scale well regarding the number of threads. Since there
are no locks, no thread will ever be halted (wait-freeness). However, with the number
of threads increasing, the change of two threads trying to remove the same marked node
at the same time increases as well. When this happens, one of the threads will have to
start traversing the list again from the head node. In terms of performance, this is an
expensive operation, especially when the list contains a large number of elements. A
reasonable expectation is that the ratio \emph{number of threads / list size} should
not be too high in order to maintain maximum performance.

\section{LockFreeTree}

The LockFreeTree uses compare-and-set instructions instead of locks.

\subsection{Design}

The tree used for the LockFreeTree is slightly different than the tree used for the
locking tree. All the values are stored in leaf nodes and the inner nodes are only used
for navigating through the tree.

To simulate the locking of a node, the same mechanism is used as with the LockFreeList,
namely marking nodes. However, in this case several pointers need to be updated, which 
cannot be done atomically. Due to this, a boolean MARK value is not sufficient. The flag
used is therefore a state. The following states are possible:
\begin{itemize}
\item CLEAN - No thread is currently performing any operation that affects this node.
\item IFLAG - A thread is currently performing an insert that affects this node.
\item DFLAG - A thread is currently performing a delete that affects this node.
\item MARKED - This node is to be deleted.
\end{itemize}

When a thread cannot set the flag to what is needed for its operation, because some other
thread has already set the flag, the current thread will perform the operations that still
have to happen. To be able to help with the operation, an info record is created that 
stores all the needed information to complete the operation.

This method does mean that possibly some work is performed several times. However, if
the original thread crashed or is very slow, it will ensure that the work will still be
done and other threads do not have to wait, but can make themselves useful.

\subsection{Performance}

There will be memory overhead in comparison to the other tree implementations for the
internal nodes in the tree, because all values are stored in leaf nodes. When a tree
contains a lot of elements, traversal to a leaf node will take longer than traversal in a
tree in which the internal nodes contain values too (assuming that the thread is not
blocked during the traversal). If at the same time there are few threads, this traversal
overhead may cause poor performance. For example, a single thread operating on a
CoarseGrainedTree containing a million elements should perform better than the same
operation on a LockFreeTree. This effect is enforced by the overhead of CAS operations of
the LockFreeTree, which do not occur in the CoarseGrainedTree and FineGrainedTree.
However, if there are enough threads in relation to the number of elements, the
LockFreeTree should perform better than all previously described data structures. After
all, the data structure is wait-free, whereas the other tree implementations require
threads to wait for each other while traversing the tree. Assuming that the traversal
overhead is not very large, the LockFreeTree should perform better than the other data
structures in most situations (when the ratio \emph{number of threads / number of
elements} is not too high).

The LockFreeList should scale very well with the number of threads. Each thread is always doing some work instead of waiting for others to finish their work, which makes an operation as fast as the fastest thread executing it. It makes no difference if there are ten or a thousand threads performing on the same subtree at the same time, because there is no locking overhead. The number of threads should only increase performance, especially in trees containing many elements. In the best case, the threads will likely spread over the different subtrees, thus not interfering with each other.

Though the same operation may be performed multiple times at the same time by different threads in case of interference, overall performance should not be affected by this redundant work (since the other option is to wait for another thread and do no work at all, which results in lost time anyway).

\begin{thebibliography}{}

\bibitem{insert}
    Binary Search Tree: Insertion
    \url{http://www.algolist.net/Data_structures/Binary_search_tree/Insertion}
    
\bibitem{delete}
    Binary Search Tree: Removal
    \url{http://www.algolist.net/Data_structures/Binary_search_tree/Removal}
  
\end{thebibliography}

\end{document}